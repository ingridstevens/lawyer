{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A Over German Citizenship Law (Modernisierung des Staatsangeh√∂rigkeitsrechts (GesEntw BReg))\n",
    "\n",
    "Source: https://www.bundestag.de/parlament/plenum/abstimmung/abstimmung?id=893\n",
    "* https://dserver.bundestag.de/btd/20/090/2009044.pdf\n",
    "* https://dserver.bundestag.de/btd/20/100/2010093.pdf\n",
    "\n",
    "Using: \n",
    "* Chroma as vectorstore\n",
    "* OpenAI Embeddings\n",
    "* OpenAI API as LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start, you will need to set the following environmental variables on your computer:\n",
    "\n",
    "**OpenAI**\n",
    "* OPENAI_API_KEY\n",
    "\n",
    "\n",
    "To add to your env variables on a mac, run this in your terminal:\n",
    "\n",
    "```\n",
    "export OPENAI_API_KEY=your_api_key\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, create a virtual environment to work in \n",
    "\n",
    "```\n",
    "conda create -n chat-with-citizenship-laws python=3.10\n",
    "conda activate chat-with-citizenship-laws\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain langchain_openai gradio chromadb pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define llm and embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "# To use a local model through LM Studio, set llm like the commented line below\n",
    "# llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Process Data & Set up Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Define the data (URLs in this case) for the vector database\n",
    "\n",
    "Note: if you want to include different information in your RAG system, this is where you can add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German Residence Laws\n",
    "\n",
    "# See: https://www.bundestag.de/parlament/plenum/abstimmung/abstimmung?id=893\n",
    "\n",
    "Gesetzentwurf =  \"https://dserver.bundestag.de/btd/20/090/2009044.pdf\"\n",
    "Beschlussempfehlung_und_Bericht = \"https://dserver.bundestag.de/btd/20/100/2010093.pdf\"\n",
    "\n",
    "# Put in an array so that we can loop over them\n",
    "urls = [Gesetzentwurf, Beschlussempfehlung_und_Bericht]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Split PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "\n",
    "for url in urls:\n",
    "    loader = PyPDFLoader(url)\n",
    "    pages += loader.load_and_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: are there any pages? (this should be non-zero)\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Create a vector database with Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorstore in Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = pages, \n",
    "    embedding=embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect your retriever to the vector store\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set up the Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Define the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "        ###INSTRUCTIONS: \n",
    "        You are polite and professional question-answering AI assistant. You must provide a helpful response to the user. \n",
    "        \n",
    "        In your response, PLEASE ALWAYS:\n",
    "          (0) Be a detail-oriented reader: read the question and context and understand both before answering\n",
    "          (1) Start your answer with a friendly tone, and reiterate the question so the user is sure you understood it\n",
    "          (2) If the context enables you to answer the question, write a detailed, helpful, and easily understandable answer with sources referenced inline. IF NOT: you can't find the answer, respond with an explanation, starting with: \"I couldn't find the information in the laws I have access to\". \n",
    "          (3) Below the answer, please list out all the referenced sources (i.e. legal paragraphs backing up your claims)\n",
    "          (4) Now you have your answer, that's amazing - review your answer to make sure it answers the question, is helpful and professional and formatted to be easily readable.\n",
    "        \n",
    "        Think step by step. \n",
    "        ###\n",
    "        \n",
    "      Answer the following question using the context provided.\n",
    "        ### Question: {question} ###\n",
    "\n",
    "        ### Context: {context} ###\n",
    "\n",
    "        \n",
    "\n",
    "        ### Helpful Answer with Sources:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # create prompt template\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Create the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:** Check that it's working by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = chain.invoke(\"What changed in German citizenship law?\")\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Set up Simple UI using Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.1: Create a function to use in Gradio\n",
    "Creating a function allows us to add this function to Gradio, and use it as a UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function get_answer which takes in a question and returns an answer\n",
    "def get_answer(question):\n",
    "    answer = chain.invoke(question)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.2: Create and run the Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iface = gr.Interface(fn=get_answer, inputs=gr.Textbox(\n",
    "    value=\"Enter your question\"),\n",
    "    live=False, \n",
    "    outputs=\"markdown\",  \n",
    "    title=\"Chat with the New German Citizenship Laws\",\n",
    "    description=\"Ask a question about German Residence Laws and get an answer from a friendly AI assistant. This assistant looks up relevant German Residence laws and answers your question.\",\n",
    "    examples=[[\"What changed in German citizenship law?\"], \n",
    "            [\"Do you need to take a citizens test before you can get citizenship?\"],\n",
    "            [\"Do I need to renounce citizenship of my home country to get German Citizenship?\"]],\n",
    "    theme=gr.themes.Soft(),\n",
    "    allow_flagging=\"never\",)\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
