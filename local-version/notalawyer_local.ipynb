{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not a Lawyer | Local\n",
    "\n",
    "The Not a Lawyer project is a project to create an AI assistant chatbot which has read German laws. It is a RAG flow, meaning that it first creates a database of legal document cunks from the provided legal documents. Then, using a semantic search, it finds the most relevant chunks to the user's question. Finally, it passes in the relevant chunks along with a user's question into a text generation model to generate an answer to the user's question.\n",
    "\n",
    "This notebook contains the local version of the chatbot. This means that the vector database is stored locally and the text generation model is stored locally. Only the embedding model is stored remotely: we're using the `HuggingFaceInferenceAPIEmbeddings` class from the `langchain` library to access the HuggingFace Hub API.\n",
    "\n",
    "## Setup\n",
    "\n",
    "* Embedding Model: Hugging Face sentence-transformers/all-MiniLM-l6-v2 -> https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub \n",
    "* Vectorstore: Chroma \n",
    "* LLM: Ollama (mixtral:8x7b-instruct-v0.1-q3_K_M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate \n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define llm and embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIRST: go to your terminal, and type `ollama run mistral`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THIS TO WORK, YOU NEED TO:\n",
    "# 1. Download ollama from ollama.ai \n",
    "# 2. Download the mistral model\n",
    "# 3. Run mistral with the following terminal command: ollama run mistral\n",
    "llm = Ollama(model='mixtral:8x7b-instruct-v0.1-q3_K_M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is not 100% local, but you can modify this code to use a local embedding model\n",
    "HF_API_KEY = os.environ.get(\"HF_API_KEY\")\n",
    "\n",
    "hf_embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=HF_API_KEY, model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")\n",
    "\n",
    "# set huggingface embeddings \n",
    "embedding = hf_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Process Data & Set up Vector Database with Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Define the data (URLs in this case) for the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German Residence Laws\n",
    "\n",
    "aufentv =  \"https://www.gesetze-im-internet.de/aufenthv/BJNR294510004.html\"\n",
    "aufenthg = \"https://www.gesetze-im-internet.de/aufenthg_2004/BJNR195010004.html\"\n",
    "urls = [aufentv, aufenthg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Split by HTML Headers\n",
    "*Note:* This is one of many ways to skin this cat. You could also use different types of splitters, e.g. by paragraphs, by sentences, by words, etc.\n",
    "\n",
    "As part of the evaluation, this needs to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs = []\n",
    "\n",
    "for url in urls:\n",
    "    html_header_splits = html_splitter.split_text_from_url(url)\n",
    "    docs += html_header_splits\n",
    "\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(html_header_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Create a vector database with Chroma\n",
    "This takes ~40 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes about 40 seconds to run\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = splits, \n",
    "    embedding=embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set up the Prompt\n",
    "I'll use a variety of Propmts to see what works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "        You are a wonderful, careful, and professional question-answering AI assistant knowledgeable in reading German law and explaining it to non-legal people. \n",
    "\n",
    "        You will be provided with a question and some legal context.        \n",
    "        \n",
    "        Please answer the question to the best of your ability using only the provided legal texts.\n",
    "\n",
    "        Below the answer, please list out all the referenced sources (i.e. legal paragraphs backing up your claims)\n",
    "\n",
    "        Let's think step by step. Here is the question, and here is the law. What is the answer?\n",
    "\n",
    "        ---- Start User Question ----\n",
    "        Question: {question}\n",
    "        ---- End User Question ----\n",
    "\n",
    "        ---- Start Law Context ----\n",
    "        Law: {context}\n",
    "        ---- End Law Context ----\n",
    "\n",
    "        If you can't find the answer in the texts provided, or if there are no texts provided, say only: \"I'm sorry, but I don't know the answer to this question.\"\n",
    "\n",
    "        Helpful Answer with Sources:\n",
    "\n",
    "        \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the retriever to use the vectorstore\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith Tags\n",
    "tags = [\"plain\",\"runnables\",\"htmlheadersplitter\", \"chroma\", \"ollama:mixtral:8x7b-instruct-v0.1-q3_K_M\", \"hf_embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser()    \n",
    ").with_config({\"tags\": tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ").with_config({\"tags\": tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the maximum duration of a residence permit?\"\n",
    "rag_chain.invoke(question)\n",
    "chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Response from the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = chain.invoke(\"How do I get a blue card?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This displays it nicely\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: MultiQueryRetreiver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm),\n",
    "    chain_type=\"stuff\", # options are \"stuff\" \"refine\" or \"map_reduce\"\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
