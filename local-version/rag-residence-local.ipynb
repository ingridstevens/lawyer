{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local: Not a Lawyer\n",
    "\n",
    "Using: \n",
    "* Chroma as vectorstore\n",
    "* Ollama as LLM\n",
    "* HF as embedding model (not quite 100% local) -> https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub \n",
    "* Try HuggingFace --> phi2 (or default to //Ollama (mistral) as LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate \n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define llm and embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THIS TO WORK, YOU NEED TO:\n",
    "# 1. Download ollama from ollama.ai \n",
    "# 2. Download the mistral model\n",
    "# 3. Run mistral with the following terminal command: ollama run mistral\n",
    "llm = Ollama(model='mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is not 100% local, but you can modify this code to use a local embedding model\n",
    "HF_API_KEY = os.environ.get(\"HF_API_KEY\")\n",
    "\n",
    "hf_embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=HF_API_KEY, model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")\n",
    "\n",
    "# set huggingface embeddings \n",
    "embedding = hf_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Process Data & Set up Vector Database with Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Define the data (URLs in this case) for the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German Residence Laws\n",
    "\n",
    "aufentv =  \"https://www.gesetze-im-internet.de/aufenthv/BJNR294510004.html\"\n",
    "aufenthg = \"https://www.gesetze-im-internet.de/aufenthg_2004/BJNR195010004.html\"\n",
    "urls = [aufentv, aufenthg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Split by HTML Headers\n",
    "*Note:* This is one of many ways to skin this cat. You could also use different types of splitters, e.g. by paragraphs, by sentences, by words, etc.\n",
    "\n",
    "As part of the evaluation, this needs to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs = []\n",
    "\n",
    "for url in urls:\n",
    "    html_header_splits = html_splitter.split_text_from_url(url)\n",
    "    docs += html_header_splits\n",
    "\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(html_header_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Create a vector database with Chroma\n",
    "This takes ~40 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes about 40 seconds to run\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = splits, \n",
    "    embedding=embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the retriever to use the vectorstore\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set up the Prompt\n",
    "I'll use a variety of Propmts to see what works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "        You are a wonderful, careful, and professional question-answering AI assistant knowledgeable in reading German law and explaining it to common people. \n",
    "        You will be provided a Question delimited by ### and some legal texts delimited by $$$.\n",
    "        \n",
    "        Please answer the question to the best of your ability using only the provided legal texts.\n",
    "\n",
    "        Below the answer, please list out all the referenced sources (i.e. legal paragraphs backing up your claims)\n",
    "\n",
    "        Let's think step by step. Here is the question, and here is the law. What is the answer?\n",
    "\n",
    "        ---- Start User Question ----\n",
    "        Question: {question}\n",
    "        ---- End User Question ----\n",
    "\n",
    "        ---- Start Law Context ----\n",
    "        Law: {context}\n",
    "        ---- End Law Context ----\n",
    "\n",
    "        If you can't find the answer in the texts provided, or if there are no texts provided, say only: \"I'm sorry, but I don't know the answer to this question.\"\n",
    "\n",
    "        Helpful Answer with Sources:\n",
    "\n",
    "        \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser()    \n",
    ").with_config({\"tags\": [\"chroma\", \"1000/200\", \"HTMLHeaderTextSplitter\", \"hf_embeddings\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the maximum duration of a residence permit?\"\n",
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ").with_config({\"tags\": [\"plain\",\"runnables\",\"htmlheadersplitter\", \"chroma\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"How do I get a blue card?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: MultiQueryRetreiver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm),\n",
    "    chain_type=\"stuff\", # options are \"stuff\" \"refine\" or \"map_reduce\"\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
