{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not a German Lawyer \n",
    "\n",
    "A Jupyter notebook to help navigate the residency law in Germany. This project uses local embeddings and models to do RAG (Retreival Augmented Generation) over the German residency law. This means that the model is living locally on the computer, the embeddings are done locally, and the querying is done locally.\n",
    "\n",
    "You can ask questions like:\n",
    "\n",
    "* What are the requirements for a Blue Card?\n",
    "* What are the requirements for a student visa?\n",
    "* What are the requirements for a work visa?\n",
    "\n",
    "I've taken the Aufenthaltsgesetz and Aufenthaltsverordnung from Gesetze im Internet as XML and using the Unstructured XML loader, I've loaded them in as a LangChain document.\n",
    "\n",
    "### Still to do\n",
    "\n",
    "* play with different ways of splitting the document into sections, different document loaders\n",
    "* use the HTML loader to load the document and split by headings\n",
    "* create an array of questions to test the model\n",
    "\n",
    "## Project Steps\n",
    "\n",
    "1. Load the XML files into a LangChain document\n",
    "2. Split the document into sections\n",
    "3. Embeddings\n",
    "4. Vector Store\n",
    "5. LLM Setup (Prompt Template & Querying)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Use LangChain Unstructured XML Loader to Load in the German Residence Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredXMLLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings, OpenAIEmbeddings, HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain.vectorstores import Chroma, Qdrant\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain   \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain.llms import Ollama, OpenAI\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Load & Split the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German Residence Law\n",
    "# source: https://www.gesetze-im-internet.de/aufenthv/BJNR294510004.html\n",
    "file = \"german-law/laws/Aufenthaltsverordnung/BJNR294510004.xml\"\n",
    "\n",
    "aufenthg = \"german-law/laws/Aufenthalt-BJNR195010004.xml\"\n",
    "\n",
    "# # load German Residence Law XML file with UnstructuredXMLLoader , mode=elements\n",
    "loader = UnstructuredXMLLoader(file_path = file)\n",
    "old_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(old_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file, aufenthg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple files into the document \n",
    "docs = []\n",
    "for file in files: \n",
    "    # load German Residence Law XML file with UnstructuredXMLLoader\n",
    "    loader = UnstructuredXMLLoader(file_path = file)\n",
    "    docs += loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recursive Character Text Splitter**\n",
    "\n",
    "Use recursive character text splitter to split texts into chunks of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with the RecursiveCharacterTextSplitter\n",
    "\n",
    "r_text_splitter = RecursiveCharacterTextSplitter(chunk_size = 10000, chunk_overlap  = 1000)\n",
    "r_texts = r_text_splitter.split_documents(docs)\n",
    "# r_texts_old = r_text_splitter.create_documents([docs[0].page_content])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r_texts_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(r_texts))\n",
    "print(type(r_texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with the CharacterTextSplitter\n",
    "\n",
    "c_text_splitter = CharacterTextSplitter(chunk_size = 1500, chunk_overlap  = 150)\n",
    "c_texts = c_text_splitter.create_documents([docs[0].page_content])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get HF_API_KEY from os \n",
    "import os\n",
    "HF_API_KEY = os.getenv(\"HF_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Create Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding mdoel BAAI/bge-small-en-v1.5 \n",
    "\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=HF_API_KEY, model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Embeddings, Chroma as vectorstore\n",
    "openai_vectorstore = Chroma.from_documents(documents = r_texts, embedding=embeddings)\n",
    "retreiver = openai_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Embeddings\n",
    "(Note: takes about 9 minutes / document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama Embeddings (openhermes2.5), Qdrant as vectorstore \n",
    "# Note: (Chroma does not work, as Ollama creates 4096-dimensional vectors and Chroma accepts 1536-dimensional vectors only)\n",
    "\n",
    "# loader = TextLoader(\"/Users/ingrid/Developer/GitHub/lawyer/README.md\")\n",
    "# docs = loader.load()\n",
    "\n",
    "# test_text_splitter = CharacterTextSplitter(chunk_size = 1500, chunk_overlap  = 150)\n",
    "# test_texts = test_text_splitter.create_documents([docs[0].page_content])\n",
    "\n",
    "# REMEMBER: set the documents= to the docs that you want to embed (this function is expensive)\n",
    "\n",
    "ollama_vectorstore = Qdrant.from_documents(\n",
    "    documents=r_texts, \n",
    "    embedding=OllamaEmbeddings(\n",
    "        model=\"llama2\",\n",
    "        show_progress=True,\n",
    "        ),\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"texts\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_retreiver = ollama_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: LLM Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily set the model to 'mistral'\n",
    "# llm = Ollama(model='llama2')\n",
    "\n",
    "# switch it to use LM Studio\n",
    "llm = OpenAI(base_url=\"http://localhost:1234/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval QA Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let the Not a Lawyer be a Not Lawyer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve prompt: \n",
    "* check that returned snippets are relevant to answering the question\n",
    "* instruct the model on the formatting of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function which takes as inputs the llm, embeddings, and outputs the result (printed)\n",
    "# ideally log as tags which llm and embeddings was used, allow me to categorize outputs as (good, not good, or comment in some ways)\n",
    "import time \n",
    "def test_llm(vectorstore, model, question):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # build prompt \n",
    "    template = \"\"\"\n",
    "        You are helpful question-answering AI assistant. You will be provided a ### Question ### and some $$$ legal texts $$$ that may be relevant. \n",
    "        \n",
    "        Start your response by providing an overview of the Question provided by the user. \n",
    "        \n",
    "\n",
    "        Below the answer, list out all the referenced sources (i.e. legal paragraphs backing up your claims)\n",
    "        \n",
    "        ### Question: {question} ###\n",
    "\n",
    "    \n",
    "        $$$ Law: {context} $$$\n",
    "\n",
    "        Let's think step by step. \n",
    "\n",
    "\n",
    "        Helpful Answer with Sources:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # create prompt template\n",
    "    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "    # set qa chain\n",
    "    qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "        Ollama(model=model), \n",
    "        retriever = vectorstore.as_retriever(),\n",
    "        chain_type=\"stuff\", # options are \"stuff\" \"refine\" or \"map_reduce\"\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "    )\n",
    "\n",
    "    # get the result\n",
    "    result = qa_chain_mr({\"query\": question})\n",
    "\n",
    "    # print the result\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    print(\"The function took\", elapsed_time, \"seconds to run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Ollama to Install the local Models You Want to Use\n",
    "\n",
    "Run the following commands in your terminal to install the models you want to use:\n",
    "\n",
    "`ollama run llama2`\n",
    "\n",
    "`ollama run mistral`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frage = \"How can I move to germany to study? I'm from the United states. I have applied, but don't know if i'll be accepted to university\"\n",
    "test_llm(ollama_vectorstore, 'llama2', frage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frage = \"How can I move to germany? I'm from the United states.\"\n",
    "test_llm(ollama_vectorstore, 'mistral', frage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frage = \"How can I move to germany? I'm from the United states.\"\n",
    "test_llm(ollama_vectorstore, 'mistral', frage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frage = \"I just got a job in Germany paying me 80,000 euros annually. What are my options for a residence permit?\"\n",
    "test_llm(ollama_vectorstore, 'mistral', frage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frage = \"What are the requirements for a Blue Card?\"\n",
    "test_llm(openai_vectorstore, 'llama2', frage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frage = \"What are the requirements for a Blue Card?\"\n",
    "test_llm(openai_vectorstore, 'openhermes2.5-mistral:7b-q5_K_M', frage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frage = \"What are the requirements for a Blue Card?\"\n",
    "test_llm(openai_vectorstore, 'mistral', frage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frage = \"What are the requirements for a Blue Card?\"\n",
    "test_llm(ollama_vectorstore, 'mistral', frage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frage = \"What are the requirements for a Blue Card?\"\n",
    "test_llm(ollama_vectorstore, 'llama2', frage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frage = \"What are the requirements for a Blue Card?\"\n",
    "test_llm(ollama_vectorstore, 'openhermes2.5-mistral:7b-q5_K_M', frage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frage = \"How can a resident of Germany obtain citizenship?\"\n",
    "test_llm(ollama_vectorstore, 'mistral', frage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_llm(ollama_vectorstore, 'llama2', frage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "\n",
    "Recursive Text Splitter\n",
    " * mistral: 19.5s\n",
    " * llama2: 26.2s\n",
    "\n",
    " Text splitter\n",
    " * mistral: 26.5s\n",
    " * llama2: 79.7s\n",
    "\n",
    " Conclusion: mistral is faster, recursive character text splitter is faster. Why? No idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a function to run the conversational retrieval chain (including memory)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_llm_inkl_memory(vectorstore, model, question):\n",
    "\n",
    "    retriever=vectorstore.as_retriever()\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        Ollama(model=model),\n",
    "        retriever=retriever,\n",
    "        memory=memory\n",
    "    )\n",
    "    result = qa({\"question\": question}) \n",
    "    print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qyery = \"can i travel outside the EU with a blue card valid for less than 6 months?\"\n",
    "\n",
    "test_llm_inkl_memory(ollama_vectorstore, 'mistral', qyery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the answer\n",
    "question = \"How do I get a bluecard?\"\n",
    "test_llm_inkl_memory(openai_vectorstore, 'mistral', question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"I don't already have a bluecard, but I just got a job offer for 100k. Can I get a bluecard?\"\n",
    "test_llm_inkl_memory(openai_vectorstore, 'mistral', question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"How do i get one if i haven't had one before?\"\n",
    "test_llm_inkl_memory(openai_vectorstore, 'mistral', question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
