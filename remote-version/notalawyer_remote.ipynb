{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not a Lawyer | Remote\n",
    "\n",
    "Using: \n",
    "* Pinecone (https://www.pinecone.io/) as vectorstore\n",
    "* OpenAI Embeddings\n",
    "* OpenAI API as LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiofiles==23.2.1\n",
      "aiohttp==3.9.1\n",
      "aiosignal==1.3.1\n",
      "altair==5.2.0\n",
      "annotated-types==0.6.0\n",
      "anyio==4.2.0\n",
      "appnope @ file:///home/conda/feedstock_root/build_artifacts/appnope_1649077682618/work\n",
      "asgiref==3.7.2\n",
      "asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1698341106958/work\n",
      "async-timeout==4.0.3\n",
      "attrs==23.2.0\n",
      "backoff==2.2.1\n",
      "bcrypt==4.1.2\n",
      "build==1.0.3\n",
      "cachetools==5.3.2\n",
      "certifi==2023.11.17\n",
      "charset-normalizer==3.3.2\n",
      "chroma-hnswlib==0.7.3\n",
      "chromadb==0.4.22\n",
      "click==8.1.7\n",
      "colorama==0.4.6\n",
      "coloredlogs==15.0.1\n",
      "comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1704278392174/work\n",
      "contourpy==1.2.0\n",
      "cycler==0.12.1\n",
      "dataclasses-json==0.6.3\n",
      "debugpy @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_563_nwtkoc/croot/debugpy_1690905063850/work\n",
      "decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work\n",
      "Deprecated==1.2.14\n",
      "distro==1.9.0\n",
      "exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1704921103267/work\n",
      "executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1698579936712/work\n",
      "fastapi==0.109.0\n",
      "ffmpy==0.3.1\n",
      "filelock==3.13.1\n",
      "flatbuffers==23.5.26\n",
      "fonttools==4.47.2\n",
      "frozenlist==1.4.1\n",
      "fsspec==2023.12.2\n",
      "google-auth==2.26.2\n",
      "googleapis-common-protos==1.62.0\n",
      "gradio==4.15.0\n",
      "gradio_client==0.8.1\n",
      "grpcio==1.60.0\n",
      "h11==0.14.0\n",
      "httpcore==1.0.2\n",
      "httptools==0.6.1\n",
      "httpx==0.26.0\n",
      "huggingface-hub==0.20.2\n",
      "humanfriendly==10.0\n",
      "idna==3.6\n",
      "importlib-metadata==6.11.0\n",
      "importlib-resources==6.1.1\n",
      "ipykernel @ file:///Users/runner/miniforge3/conda-bld/ipykernel_1705418038222/work\n",
      "ipython @ file:///home/conda/feedstock_root/build_artifacts/ipython_1704718870316/work\n",
      "jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1696326070614/work\n",
      "Jinja2==3.1.3\n",
      "jsonpatch==1.33\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.21.1\n",
      "jsonschema-specifications==2023.12.1\n",
      "jupyter_client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1699283905679/work\n",
      "jupyter_core @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_782yoyc_98/croot/jupyter_core_1698937318631/work\n",
      "kiwisolver==1.4.5\n",
      "kubernetes==29.0.0\n",
      "langchain==0.1.1\n",
      "langchain-community==0.0.13\n",
      "langchain-core==0.1.13\n",
      "langchain-openai==0.0.3\n",
      "langsmith==0.0.83\n",
      "lxml==5.1.0\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==2.1.4\n",
      "marshmallow==3.20.2\n",
      "matplotlib==3.8.2\n",
      "matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work\n",
      "mdurl==0.1.2\n",
      "mmh3==4.1.0\n",
      "monotonic==1.6\n",
      "mpmath==1.3.0\n",
      "multidict==6.0.4\n",
      "mypy-extensions==1.0.0\n",
      "nest_asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1705352640985/work\n",
      "numpy==1.26.3\n",
      "oauthlib==3.2.2\n",
      "onnxruntime==1.16.3\n",
      "openai==1.8.0\n",
      "opentelemetry-api==1.22.0\n",
      "opentelemetry-exporter-otlp-proto-common==1.22.0\n",
      "opentelemetry-exporter-otlp-proto-grpc==1.22.0\n",
      "opentelemetry-instrumentation==0.43b0\n",
      "opentelemetry-instrumentation-asgi==0.43b0\n",
      "opentelemetry-instrumentation-fastapi==0.43b0\n",
      "opentelemetry-proto==1.22.0\n",
      "opentelemetry-sdk==1.22.0\n",
      "opentelemetry-semantic-conventions==0.43b0\n",
      "opentelemetry-util-http==0.43b0\n",
      "orjson==3.9.12\n",
      "overrides==7.5.0\n",
      "packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1696202382185/work\n",
      "pandas==2.2.0\n",
      "parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work\n",
      "pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1667297516076/work\n",
      "pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work\n",
      "pillow==10.2.0\n",
      "pinecone-client==3.0.1\n",
      "platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1701708255999/work\n",
      "posthog==3.3.2\n",
      "prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1702399386289/work\n",
      "protobuf==4.25.2\n",
      "psutil @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_1310b568-21f4-4cb0-b0e3-2f3d31e39728k9coaga5/croots/recipe/psutil_1656431280844/work\n",
      "ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\n",
      "pulsar-client==3.4.0\n",
      "pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work\n",
      "pyasn1==0.5.1\n",
      "pyasn1-modules==0.3.0\n",
      "pydantic==2.5.3\n",
      "pydantic_core==2.14.6\n",
      "pydub==0.25.1\n",
      "Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1700607939962/work\n",
      "pyparsing==3.1.1\n",
      "PyPika==0.48.9\n",
      "pyproject_hooks==1.0.0\n",
      "python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work\n",
      "python-dotenv==1.0.0\n",
      "python-multipart==0.0.6\n",
      "pytz==2023.3.post1\n",
      "PyYAML==6.0.1\n",
      "pyzmq @ file:///private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_43pxpbos3z/croot/pyzmq_1705605108344/work\n",
      "referencing==0.32.1\n",
      "regex==2023.12.25\n",
      "requests==2.31.0\n",
      "requests-oauthlib==1.3.1\n",
      "rich==13.7.0\n",
      "rpds-py==0.17.1\n",
      "rsa==4.9\n",
      "ruff==0.1.14\n",
      "semantic-version==2.10.0\n",
      "shellingham==1.5.4\n",
      "six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work\n",
      "sniffio==1.3.0\n",
      "SQLAlchemy==2.0.25\n",
      "stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work\n",
      "starlette==0.35.1\n",
      "sympy==1.12\n",
      "tenacity==8.2.3\n",
      "tiktoken==0.5.2\n",
      "tokenizers==0.15.0\n",
      "tomli==2.0.1\n",
      "tomlkit==0.12.0\n",
      "toolz==0.12.0\n",
      "tornado @ file:///private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_3a5nrn2jeh/croot/tornado_1696936974091/work\n",
      "tqdm==4.66.1\n",
      "traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1704212992681/work\n",
      "typer==0.9.0\n",
      "typing-inspect==0.9.0\n",
      "typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1702176139754/work\n",
      "tzdata==2023.4\n",
      "urllib3==2.1.0\n",
      "uvicorn==0.26.0\n",
      "uvloop==0.19.0\n",
      "watchfiles==0.21.0\n",
      "wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1704731205417/work\n",
      "websocket-client==1.7.0\n",
      "websockets==11.0.3\n",
      "wrapt==1.16.0\n",
      "yarl==1.9.4\n",
      "zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1695255097490/work\n"
     ]
    }
   ],
   "source": [
    "! pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start, you will need to set the following environmental variables on your computer:\n",
    "\n",
    "**Pinecone**\n",
    "* PINECONE_API_KEY\n",
    "* PINECONE_ENVIRONMENT\n",
    "* PINECONE_INDEX\n",
    "\n",
    "**OpenAI**\n",
    "* OPENAI_API_KEY\n",
    "\n",
    "\n",
    "To add to your env variables on a mac, run this in your terminal:\n",
    "\n",
    "```\n",
    "export PINECONE_API_KEY=your_api_key PINECONE_ENVIRONMENT=env PINECONE_INDEX=index OPENAI_API_KEY=your_api_key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain langchain_openai gradio lxml pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define llm and embedding models\n",
    "I'll use two variants: OpenAI and local model via LM Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0, openai_api_key=\"\")\n",
    "\n",
    "# To use a local model through LM Studio, set llm like the commented line below\n",
    "# llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(openai_api_key=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Process Data & Set up Vector Database with Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Define the data (URLs in this case) for the vector database\n",
    "\n",
    "Note: if you want to include different information in your RAG system, this is where you can add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German Residence Laws\n",
    "\n",
    "aufentv =  \"https://www.gesetze-im-internet.de/aufenthv/BJNR294510004.html\"\n",
    "aufenthg = \"https://www.gesetze-im-internet.de/aufenthg_2004/BJNR195010004.html\"\n",
    "\n",
    "# Put in an array so that we can loop over them\n",
    "urls = [aufentv, aufenthg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Split by HTML Headers\n",
    "We split by HTML headers, as the hierarchy of headers is a good indicator of the structure of the document. We use the following headers: h1, h2, h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs = []\n",
    "\n",
    "for url in urls:\n",
    "    html_header_splits = html_splitter.split_text_from_url(url)\n",
    "    docs += html_header_splits\n",
    "\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(html_header_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: are there any splits (this should be non-zero)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Create a vector database with Pinecone\n",
    "You need to create a pinecone account, and create an index: https://app.pinecone.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First time here? Upsert the documents to the vector store (you only need to do this once)\n",
    "\n",
    "# vectorstore = Pinecone.from_documents(\n",
    "#     documents=splits, embedding=OpenAIEmbeddings(), index_name=\"lawyer\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you already have your documents in the vector store? Then just load it:\n",
    "vectorstore = Pinecone.from_existing_index(\"lawyer\", OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect your retriever to the vector store\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set up the Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Define the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "        ###INSTRUCTIONS: \n",
    "        You are polite and professional question-answering AI assistant. You must provide a helpful response to the user. \n",
    "        \n",
    "        In your response, PLEASE ALWAYS:\n",
    "          (0) Be a detail-oriented reader: read the question and context and understand both before answering\n",
    "          (1) Start your answer with a friendly tone, and reiterate the question so the user is sure you understood it\n",
    "          (2) If the context enables you to answer the question, write a detailed, helpful, and easily understandable answer with sources referenced inline. IF NOT: you can't find the answer, respond with an explanation, starting with: \"I couldn't find the information in the laws I have access to\". \n",
    "          (3) Below the answer, please list out all the referenced sources (i.e. legal paragraphs backing up your claims)\n",
    "          (4) Now you have your answer, that's amazing - review your answer to make sure it answers the question, is helpful and professional and formatted to be easily readable.\n",
    "        \n",
    "        Think step by step. \n",
    "        ###\n",
    "        \n",
    "      Answer the following question using the context provided.\n",
    "        ### Question: {question} ###\n",
    "\n",
    "        ### Context: {context} ###\n",
    "\n",
    "        \n",
    "\n",
    "        ### Helpful Answer with Sources:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # create prompt template\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Create the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that it's working by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = chain.invoke(\"How do I get a blue card in Germany?\")\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Set up Simple UI using Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.1: Create a function to use in Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that takes a question and returns an answer is a good idea since \n",
    "# I'll make a gradio UI in the next cell, and this simplifies it\n",
    "def get_answer(question):\n",
    "    answer = chain.invoke(question)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.2: Create and run the Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iface = gr.Interface(fn=get_answer, inputs=gr.Textbox(\n",
    "    value=\"Enter your question\"),\n",
    "    live=True, \n",
    "    outputs=\"markdown\",  \n",
    "    title=\"LLM Augmented Q&A over German Residence Laws\",\n",
    "    description=\"Ask a question about German Residence Laws and get an answer from a friendly AI assistant. This assistant looks up relevant German Residence laws and answers your question.\",\n",
    "    examples=[[\"How do I get a blue card in Germany?\"], \n",
    "            [\"How long can I stay in Germany with a tourist visa?\"],\n",
    "            [\"How do I get a work visa in Germany?\"],[\"I want to work in Germany, what visa do I need?\"],\n",
    "            [\"I am a student in Germany, can I work?\"]],\n",
    "    theme=gr.themes.Soft(),\n",
    "    allow_flagging=\"never\",)\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
