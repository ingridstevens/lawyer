{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not a Lawyer | Remote\n",
    "\n",
    "Using: \n",
    "* Pinecone (https://www.pinecone.io/) as vectorstore\n",
    "* OpenAI Embeddings\n",
    "* OpenAI API as LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start, you will need to set the following environmental variables on your computer:\n",
    "\n",
    "**Pinecone**\n",
    "* PINECONE_API_KEY\n",
    "* PINECONE_ENVIRONMENT\n",
    "* PINECONE_INDEX\n",
    "\n",
    "**OpenAI**\n",
    "* OPENAI_API_KEY\n",
    "\n",
    "\n",
    "To add to your env variables on a mac, run this in your terminal:\n",
    "\n",
    "```\n",
    "export PINECONE_API_KEY=your_api_key PINECONE_ENVIRONMENT=env PINECONE_INDEX=index OPENAI_API_KEY=your_api_key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from langchain.document_loaders import UnstructuredXMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA   \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define llm and embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Process Data & Set up Vector Database with Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Define the data (URLs in this case) for the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German Residence Laws\n",
    "\n",
    "aufentv =  \"https://www.gesetze-im-internet.de/aufenthv/BJNR294510004.html\"\n",
    "aufenthg = \"https://www.gesetze-im-internet.de/aufenthg_2004/BJNR195010004.html\"\n",
    "\n",
    "# Put in an array so that we can loop over them\n",
    "urls = [aufentv, aufenthg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Split by HTML Headers\n",
    "We split by HTML headers, as the hierarchy of headers is a good indicator of the structure of the document. We use the following headers: h1, h2, h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs = []\n",
    "\n",
    "for url in urls:\n",
    "    html_header_splits = html_splitter.split_text_from_url(url)\n",
    "    docs += html_header_splits\n",
    "\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(html_header_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: are there any splits (this should be non-zero)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Create a vector database with Pinecone\n",
    "You need to create a pinecone account, and create an index: https://app.pinecone.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First time here? Upsert the documents to the vector store (you only need to do this once)\n",
    "\n",
    "# vectorstore = Pinecone.from_documents(\n",
    "#     documents=splits, embedding=OpenAIEmbeddings(), index_name=\"lawyer\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you already have your documents in the vector store? Then just load it:\n",
    "vectorstore = Pinecone.from_existing_index(\"lawyer\", OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect your retriever to the vector store\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set up the Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Define the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "        ###INSTRUCTIONS: \n",
    "        You are polite and professional question-answering AI assistant. You must provide a helpful response to the user. \n",
    "        \n",
    "        In your response, PLEASE ALWAYS:\n",
    "          (0) Be a detail-oriented reader: read the question and context and understand both before answering\n",
    "          (1) Start your answer with a friendly tone, and reiterate the question so the user is sure you understood it\n",
    "          (2) If the context enables you to answer the question, write a detailed, helpful, and easily understandable answer with sources referenced inline. IF NOT: you can't find the answer, respond with an explanation, starting with: \"I couldn't find the information in the laws I have access to\". \n",
    "          (3) Below the answer, please list out all the referenced sources (i.e. legal paragraphs backing up your claims)\n",
    "          (4) Now you have your answer, that's amazing - review your answer to make sure it answers the question, is helpful and professional and formatted to be easily readable.\n",
    "        \n",
    "        Think step by step. \n",
    "        ###\n",
    "        \n",
    "      Answer the following question using the context provided.\n",
    "        ### Question: {question} ###\n",
    "\n",
    "        ### Context: {context} ###\n",
    "\n",
    "        \n",
    "\n",
    "        ### Helpful Answer with Sources:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # create prompt template\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Create the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that it's working by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = chain.invoke(\"How do I get a blue card in Germany?\")\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Set up Simple UI using Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.1: Create a function to use in Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that takes a question and returns an answer is a good idea since \n",
    "# I'll make a gradio UI in the next cell, and this simplifies it\n",
    "def get_answer(question):\n",
    "    answer = chain.invoke(question)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.2: Create and run the Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iface = gr.Interface(fn=get_answer, inputs=gr.Textbox(\n",
    "    value=\"Enter your question\"), \n",
    "        outputs=\"markdown\",  \n",
    "        title=\"LLM Augmented Q&A over German Residence Laws\",\n",
    "        description=\"Ask a question about German Residence Laws and get an answer from a friendly AI assistant. This assistant looks up relevant German Residence laws and answers your question.\",\n",
    "        examples=[[\"How do I get a blue card in Germany?\"], \n",
    "                [\"How long can I stay in Germany with a tourist visa?\"],\n",
    "                [\"How do I get a work visa in Germany?\"],[\"I want to work in Germany, what visa do I need?\"],\n",
    "                [\"I am a student in Germany, can I work?\"]],\n",
    "    theme=gr.themes.Soft(),\n",
    "    allow_flagging=\"never\",)\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
