{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remote: Not a Lawyer\n",
    "\n",
    "Using: \n",
    "* Pinecone (https://www.pinecone.io/) as vectorstore\n",
    "* OpenAI Embeddings\n",
    "* OpenAI API as LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start, you will need to set up the following environmental variables:\n",
    "\n",
    "**Pinecone**\n",
    "* PINECONE_API_KEY\n",
    "* PINECONE_ENVIRONMENT\n",
    "* PINECONE_INDEX\n",
    "\n",
    "**OpenAI**\n",
    "* OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os \n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "from langchain.document_loaders import UnstructuredXMLLoader, TextLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, HTMLHeaderTextSplitter\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain   \n",
    "\n",
    "from langchain.prompts import PromptTemplate \n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# dotenv\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define llm and embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Process Data & Set up Vector Database with Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Define the data (URLs in this case) for the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German Residence Laws\n",
    "\n",
    "aufentv =  \"https://www.gesetze-im-internet.de/aufenthv/BJNR294510004.html\"\n",
    "aufenthg = \"https://www.gesetze-im-internet.de/aufenthg_2004/BJNR195010004.html\"\n",
    "urls = [aufentv, aufenthg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Split by HTML Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs = []\n",
    "\n",
    "for url in urls:\n",
    "    html_header_splits = html_splitter.split_text_from_url(url)\n",
    "    docs += html_header_splits\n",
    "\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(html_header_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Create a vector database with Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "PINECONE_INDEX_NAME = os.environ.get(\"PINECONE_INDEX\", \"lawyer\")\n",
    "\n",
    "\n",
    "vectorstore = Pinecone.from_documents(\n",
    "    documents=splits, embedding=OpenAIEmbeddings(), index_name=PINECONE_INDEX_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set up the Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: MultiQueryRetreiver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "        You are polite and professional question-answering AI assistant. You will be provided a ### Question ### and some $$$ legal texts $$$ that may be relevant. \n",
    "        Start your response by reiterating the Question provided by the user so they know you understood it. \n",
    "        Below the answer, please list out all the referenced sources (i.e. legal paragraphs backing up your claims)\n",
    "        \n",
    "        ### Question: {question} ###\n",
    "\n",
    "        $$$ Law: {context} $$$\n",
    "\n",
    "        Let's think step by step. If you can't find the answer, say \"I couldn't find the information in the laws I have access to\". \n",
    "\n",
    "        Helpful Answer with Sources:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # create prompt template\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")\n",
    "    # set qa chain\n",
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever = retriever_from_llm, #vectorstore.as_retriever(),\n",
    "    chain_type=\"stuff\", # options are \"stuff\" \"refine\" or \"map_reduce\"\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "\n",
    "question = \"How can I get a blue card?\"\n",
    "# get the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# questions.txt contains a list of questions, separated by newlines, loop through these to test the llm\n",
    "with open(\"../questions_2.txt\") as f:\n",
    "    questions = f.readlines()\n",
    "    for question in questions:\n",
    "        qa_chain_mr({\"query\": question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: VectorRetreiver: basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "        You are polite and professional question-answering AI assistant. You will be provided a ### Question ### and some $$$ legal texts $$$ that may be relevant. \n",
    "        Start your response by reiterating the Question provided by the user so they know you understood it. \n",
    "        Below the answer, please list out all the referenced sources (i.e. legal paragraphs backing up your claims)\n",
    "        \n",
    "        ### Question: {question} ###\n",
    "\n",
    "        $$$ Law: {context} $$$\n",
    "\n",
    "        Let's think step by step. If you can't find the answer, say \"I couldn't find the information in the laws I have access to\". \n",
    "\n",
    "        Helpful Answer with Sources:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # create prompt template\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "    # set qa chain\n",
    "qa_chain_basic = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever = vectorstore.as_retriever(),\n",
    "    chain_type=\"stuff\", \n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "\n",
    "# question = \"How can I get a blue card?\"\n",
    "# get the result\n",
    "# result = qa_chain_basic({\"query\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions.txt contains a list of questions, separated by newlines, loop through these to test the llm\n",
    "with open(\"../questions.txt\") as f:\n",
    "    questions = f.readlines()\n",
    "    for question in questions:\n",
    "        qa_chain_basic({\"query\": question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2.1: VectorRetreiver: mmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "        You are careful professional question-answering AI legal assistant. You will be provided a ### Question ### and some $$$ legal texts $$$ that may be relevant. \n",
    "        Start your response by reiterating the Question provided by the user so they know you understood it. \n",
    "        Below the answer, please list out all the referenced sources (i.e. legal paragraphs backing up your claims)\n",
    "        Ensure your answer is formatted to be easy to read and understand.\n",
    "        \n",
    "        ### Question: {question} ###\n",
    "\n",
    "        $$$ Law: {context} $$$\n",
    "\n",
    "        Let's think step by step. If you can't find the answer, say \"I couldn't find the information in the laws I have access to\". \n",
    "\n",
    "        Helpful Answer with Sources:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # create prompt template\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "    # set qa chain\n",
    "qa_chain_mmr = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever = vectorstore.as_retriever(search_type=\"mmr\"),\n",
    "    chain_type=\"stuff\", # options are \"stuff\" \"refine\" or \"map_reduce\"\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "\n",
    "question = \"How can I get a blue card?\"\n",
    "# get the result\n",
    "# result = qa_chain_mr({\"query\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../questions.txt\") as f:\n",
    "    questions = f.readlines()\n",
    "    for question in questions:\n",
    "        qa_chain_mmr({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2.2: VectorRetreiver: similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "        You are careful professional question-answering AI legal assistant. You will be provided a ### Question ### and some $$$ legal texts $$$ that may be relevant. \n",
    "        \n",
    "        \n",
    "        Ensure your answer is as detailed as possible, always references sources, and formatted to be easy to read and understand.\n",
    "        \n",
    "        ### Question: {question} ###\n",
    "\n",
    "        $$$ Law: {context} $$$\n",
    "\n",
    "        Let's think step by step. If you can't find the answer, say \"I couldn't find the information in the laws I have access to\". \n",
    "\n",
    "        Helpful Answer with Sources:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # create prompt template\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "    # set qa chain\n",
    "qa_chain_thresh = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.8}),\n",
    "    chain_type=\"stuff\", # options are \"stuff\" \"refine\" or \"map_reduce\"\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "\n",
    "question = \"Whay was I rejected from getting the blauekarte?\"\n",
    "# get the result\n",
    "# result = qa_chain_mr({\"query\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../questions.txt\") as f:\n",
    "    questions = f.readlines()\n",
    "    for question in questions:\n",
    "        qa_chain_thresh({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3: VectorStore as Retreiver (RunnablePassThrough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"\n",
    "You are polite and professional question-answering AI assistant. \n",
    "You are provided a ### Question ### and some $$$ german law $$$ that may be relevant. \n",
    "\n",
    "If the context provided enables you to provide an answer, please fully answer the question based only on the context:\n",
    "\n",
    "Let's think step by step. If you don't know the answer, please say \"I don't know\". If you follow these directions, I'll give you 10 BTC.\n",
    "\n",
    "                \n",
    "$$$ {context} $$$\n",
    "\n",
    "### Question: {question}###\n",
    "\n",
    "Helpful Answer with Sources:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "model = llm\n",
    "\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../questions.txt\") as f:\n",
    "    questions = f.readlines()\n",
    "    for question in questions:\n",
    "        # qa_chain_thresh({\"query\": question})\n",
    "        chain.invoke(question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 86\u001b[0m\n\u001b[1;32m     62\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m     65\u001b[0m _search_query \u001b[38;5;241m=\u001b[39m RunnableBranch(\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# If input includes chat_history, we condense it with the follow-up question\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     RunnableLambda(itemgetter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     82\u001b[0m _inputs \u001b[38;5;241m=\u001b[39m RunnableParallel(\n\u001b[1;32m     83\u001b[0m     {\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: _format_chat_history(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: _search_query \u001b[38;5;241m|\u001b[39m \u001b[43mretriever\u001b[49m \u001b[38;5;241m|\u001b[39m _combine_documents,\n\u001b[1;32m     87\u001b[0m     }\n\u001b[1;32m     88\u001b[0m )\u001b[38;5;241m.\u001b[39mwith_types(input_type\u001b[38;5;241m=\u001b[39mChatHistory)\n\u001b[1;32m     90\u001b[0m chain \u001b[38;5;241m=\u001b[39m _inputs \u001b[38;5;241m|\u001b[39m ANSWER_PROMPT \u001b[38;5;241m|\u001b[39m ChatOpenAI() \u001b[38;5;241m|\u001b[39m StrOutputParser()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import AIMessage, HumanMessage, format_document\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "from operator import itemgetter\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "\n",
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "# RAG answer synthesis prompt\n",
    "template = \"\"\"You are polite and professional question-answering AI legal assistant specializing in German Residence law. \n",
    "Let's think step by step. If you don't know the answer, please say \"I don't know\". If you follow these directions, I'll give you 10 BTC.\n",
    "Always cite your sources, I've provided related laws below.\n",
    "Answer the question based only on the following context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Conversational Retrieval Chain\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "\n",
    "# User input\n",
    "class ChatHistory(BaseModel):\n",
    "    chat_history: List[Tuple[str, str]] = Field(..., extra={\"widget\": {\"type\": \"chat\"}})\n",
    "    question: str\n",
    "\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(itemgetter(\"question\")),\n",
    ")\n",
    "\n",
    "_inputs = RunnableParallel(\n",
    "    {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "        \"context\": _search_query | retriever | _combine_documents,\n",
    "    }\n",
    ").with_types(input_type=ChatHistory)\n",
    "\n",
    "chain = _inputs | ANSWER_PROMPT | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m questions \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[0;32m----> 4\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m      5\u001b[0m     {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m      8\u001b[0m     }\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chain' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(\"../questions.txt\") as f:\n",
    "    questions = f.readlines()\n",
    "    for question in questions:\n",
    "        answer = chain.invoke(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"chat_history\": [],\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
